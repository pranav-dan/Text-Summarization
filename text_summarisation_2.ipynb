{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import requests\n",
    "#from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#website_link = input(\"Enter the blog link: \")\n",
    "#website_data = requests.get(website_link)\n",
    "#website_data=website_data.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#soup = BeautifulSoup(website_data,[\"html.parser\",\"lxml\"])\n",
    "#heading = soup.title\n",
    "#print(heading.string.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for para in soup.find_all(\"p\"):\n",
    "#    print(para.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Summarization is the task of condensing a piece of text to a shorter version, reducing the size of the initial text while at the same time preserving key informational elements and the meaning of content. Since manual text summarization is a time expensive and generally laborious task, the automatization of the task is gaining increasing popularity and therefore constitutes a strong motivation for academic research.\\nThere are important applications for text summarization in various NLP related tasks such as text classification, question answering, legal texts summarization, news summarization, and headline generation. Moreover, the generation of summaries can be integrated into these systems as an intermediate stage which helps to reduce the length of the document.\\nIn the big data era, there has been an explosion in the amount of text data from a variety of sources. This volume of text is an inestimable source of information and knowledge which needs to be effectively summarized to be useful. This increasing availability of documents has demanded exhaustive research in the NLP area for automatic text summarization. Automatic text summarization is the task of producing a concise and fluent summary without any human help while preserving the meaning of the original text document.\\nIt is very challenging, because when we as humans summarize a piece of text, we usually read it entirely to develop our understanding, and then write a summary highlighting its main points. Since computers lack human knowledge and language capability, it makes automatic text summarization a very difficult and non-trivial task.\\nVarious models based on machine learning have been proposed for this task. Most of these approaches model this problem as a classification problem which outputs whether to include a sentence in the summary or not. Other approaches have used topic information, Latent Semantic Analysis (LSA), Sequence to Sequence models, Reinforcement Learning and Adversarial processes.\\nIn general, there are two different approaches for automatic summarization: extraction and abstraction.The extractive approach involves picking up the most important phrases and lines from the documents. It then combines all the important lines to create the summary. So, in this case, every line and word of the summary actually belongs to the original document which is summarized.The abstractive approach involves summarization based on deep learning. So, it uses new phrases and terms, different from the actual document, keeping the points the same, just like how we actually summarize. So, it is much harder than the extractive approach.\\nIt has been observed that extractive summaries sometimes work better than the abstractive ones probably because extractive ones don’t require natural language generations and semantic representations. '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blog_text = \"\"\"Summarization is the task of condensing a piece of text to a shorter version, reducing the size of the initial text while at the same time preserving key informational elements and the meaning of content. Since manual text summarization is a time expensive and generally laborious task, the automatization of the task is gaining increasing popularity and therefore constitutes a strong motivation for academic research.\n",
    "There are important applications for text summarization in various NLP related tasks such as text classification, question answering, legal texts summarization, news summarization, and headline generation. Moreover, the generation of summaries can be integrated into these systems as an intermediate stage which helps to reduce the length of the document.\n",
    "In the big data era, there has been an explosion in the amount of text data from a variety of sources. This volume of text is an inestimable source of information and knowledge which needs to be effectively summarized to be useful. This increasing availability of documents has demanded exhaustive research in the NLP area for automatic text summarization. Automatic text summarization is the task of producing a concise and fluent summary without any human help while preserving the meaning of the original text document.\n",
    "It is very challenging, because when we as humans summarize a piece of text, we usually read it entirely to develop our understanding, and then write a summary highlighting its main points. Since computers lack human knowledge and language capability, it makes automatic text summarization a very difficult and non-trivial task.\n",
    "Various models based on machine learning have been proposed for this task. Most of these approaches model this problem as a classification problem which outputs whether to include a sentence in the summary or not. Other approaches have used topic information, Latent Semantic Analysis (LSA), Sequence to Sequence models, Reinforcement Learning and Adversarial processes.\n",
    "In general, there are two different approaches for automatic summarization: extraction and abstraction.The extractive approach involves picking up the most important phrases and lines from the documents. It then combines all the important lines to create the summary. So, in this case, every line and word of the summary actually belongs to the original document which is summarized.The abstractive approach involves summarization based on deep learning. So, it uses new phrases and terms, different from the actual document, keeping the points the same, just like how we actually summarize. So, it is much harder than the extractive approach.\n",
    "It has been observed that extractive summaries sometimes work better than the abstractive ones probably because extractive ones don’t require natural language generations and semantic representations. \"\"\"\n",
    "blog_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Summarization is the task of condensing a piece of text to a shorter version, reducing the size of the initial text while at the same time preserving key informational elements and the meaning of content.',\n",
       " 'Since manual text summarization is a time expensive and generally laborious task, the automatization of the task is gaining increasing popularity and therefore constitutes a strong motivation for academic research.',\n",
       " 'There are important applications for text summarization in various NLP related tasks such as text classification, question answering, legal texts summarization, news summarization, and headline generation.',\n",
       " 'Moreover, the generation of summaries can be integrated into these systems as an intermediate stage which helps to reduce the length of the document.',\n",
       " 'In the big data era, there has been an explosion in the amount of text data from a variety of sources.',\n",
       " 'This volume of text is an inestimable source of information and knowledge which needs to be effectively summarized to be useful.',\n",
       " 'This increasing availability of documents has demanded exhaustive research in the NLP area for automatic text summarization.',\n",
       " 'Automatic text summarization is the task of producing a concise and fluent summary without any human help while preserving the meaning of the original text document.',\n",
       " 'It is very challenging, because when we as humans summarize a piece of text, we usually read it entirely to develop our understanding, and then write a summary highlighting its main points.',\n",
       " 'Since computers lack human knowledge and language capability, it makes automatic text summarization a very difficult and non-trivial task.',\n",
       " 'Various models based on machine learning have been proposed for this task.',\n",
       " 'Most of these approaches model this problem as a classification problem which outputs whether to include a sentence in the summary or not.',\n",
       " 'Other approaches have used topic information, Latent Semantic Analysis (LSA), Sequence to Sequence models, Reinforcement Learning and Adversarial processes.',\n",
       " 'In general, there are two different approaches for automatic summarization: extraction and abstraction.The extractive approach involves picking up the most important phrases and lines from the documents.',\n",
       " 'It then combines all the important lines to create the summary.',\n",
       " 'So, in this case, every line and word of the summary actually belongs to the original document which is summarized.The abstractive approach involves summarization based on deep learning.',\n",
       " 'So, it uses new phrases and terms, different from the actual document, keeping the points the same, just like how we actually summarize.',\n",
       " 'So, it is much harder than the extractive approach.',\n",
       " 'It has been observed that extractive summaries sometimes work better than the abstractive ones probably because extractive ones don’t require natural language generations and semantic representations.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "#Sentence tokenization\n",
    "sentence_list=nltk.sent_tokenize(blog_text)\n",
    "sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Summarization', 'is', 'the', 'task', 'of', 'condensing', 'a', 'piece', 'of', 'text', 'to', 'a', 'shorter', 'version', 'reducing', 'the', 'size', 'of', 'the', 'initial', 'text', 'while', 'at', 'the', 'same', 'time', 'preserving', 'key', 'informational', 'elements', 'and', 'the', 'meaning', 'of', 'content'], ['Since', 'manual', 'text', 'summarization', 'is', 'a', 'time', 'expensive', 'and', 'generally', 'laborious', 'task', 'the', 'automatization', 'of', 'the', 'task', 'is', 'gaining', 'increasing', 'popularity', 'and', 'therefore', 'constitutes', 'a', 'strong', 'motivation', 'for', 'academic', 'research'], ['There', 'are', 'important', 'applications', 'for', 'text', 'summarization', 'in', 'various', 'NLP', 'related', 'tasks', 'such', 'as', 'text', 'classification', 'question', 'answering', 'legal', 'texts', 'summarization', 'news', 'summarization', 'and', 'headline', 'generation'], ['Moreover', 'the', 'generation', 'of', 'summaries', 'can', 'be', 'integrated', 'into', 'these', 'systems', 'as', 'an', 'intermediate', 'stage', 'which', 'helps', 'to', 'reduce', 'the', 'length', 'of', 'the', 'document'], ['In', 'the', 'big', 'data', 'era', 'there', 'has', 'been', 'an', 'explosion', 'in', 'the', 'amount', 'of', 'text', 'data', 'from', 'a', 'variety', 'of', 'sources'], ['This', 'volume', 'of', 'text', 'is', 'an', 'inestimable', 'source', 'of', 'information', 'and', 'knowledge', 'which', 'needs', 'to', 'be', 'effectively', 'summarized', 'to', 'be', 'useful'], ['This', 'increasing', 'availability', 'of', 'documents', 'has', 'demanded', 'exhaustive', 'research', 'in', 'the', 'NLP', 'area', 'for', 'automatic', 'text', 'summarization'], ['Automatic', 'text', 'summarization', 'is', 'the', 'task', 'of', 'producing', 'a', 'concise', 'and', 'fluent', 'summary', 'without', 'any', 'human', 'help', 'while', 'preserving', 'the', 'meaning', 'of', 'the', 'original', 'text', 'document'], ['It', 'is', 'very', 'challenging', 'because', 'when', 'we', 'as', 'humans', 'summarize', 'a', 'piece', 'of', 'text', 'we', 'usually', 'read', 'it', 'entirely', 'to', 'develop', 'our', 'understanding', 'and', 'then', 'write', 'a', 'summary', 'highlighting', 'its', 'main', 'points'], ['Since', 'computers', 'lack', 'human', 'knowledge', 'and', 'language', 'capability', 'it', 'makes', 'automatic', 'text', 'summarization', 'a', 'very', 'difficult', 'and', 'nontrivial', 'task'], ['Various', 'models', 'based', 'on', 'machine', 'learning', 'have', 'been', 'proposed', 'for', 'this', 'task'], ['Most', 'of', 'these', 'approaches', 'model', 'this', 'problem', 'as', 'a', 'classification', 'problem', 'which', 'outputs', 'whether', 'to', 'include', 'a', 'sentence', 'in', 'the', 'summary', 'or', 'not'], ['Other', 'approaches', 'have', 'used', 'topic', 'information', 'Latent', 'Semantic', 'Analysis', 'LSA', 'Sequence', 'to', 'Sequence', 'models', 'Reinforcement', 'Learning', 'and', 'Adversarial', 'processes'], ['In', 'general', 'there', 'are', 'two', 'different', 'approaches', 'for', 'automatic', 'summarization', 'extraction', 'and', 'abstractionThe', 'extractive', 'approach', 'involves', 'picking', 'up', 'the', 'most', 'important', 'phrases', 'and', 'lines', 'from', 'the', 'documents'], ['It', 'then', 'combines', 'all', 'the', 'important', 'lines', 'to', 'create', 'the', 'summary'], ['So', 'in', 'this', 'case', 'every', 'line', 'and', 'word', 'of', 'the', 'summary', 'actually', 'belongs', 'to', 'the', 'original', 'document', 'which', 'is', 'summarizedThe', 'abstractive', 'approach', 'involves', 'summarization', 'based', 'on', 'deep', 'learning'], ['So', 'it', 'uses', 'new', 'phrases', 'and', 'terms', 'different', 'from', 'the', 'actual', 'document', 'keeping', 'the', 'points', 'the', 'same', 'just', 'like', 'how', 'we', 'actually', 'summarize'], ['So', 'it', 'is', 'much', 'harder', 'than', 'the', 'extractive', 'approach'], ['It', 'has', 'been', 'observed', 'that', 'extractive', 'summaries', 'sometimes', 'work', 'better', 'than', 'the', 'abstractive', 'ones', 'probably', 'because', 'extractive', 'ones', 'dont', 'require', 'natural', 'language', 'generations', 'and', 'semantic', 'representations']]\n"
     ]
    }
   ],
   "source": [
    "change_sentence_list = sentence_list.copy()\n",
    "#creatting deep copy of sentence tokenizing list\n",
    "\n",
    "for i in range(0,len(change_sentence_list)):\n",
    "    change_sentence_list[i] = re.sub(r'[^\\w\\s]','',change_sentence_list[i])\n",
    "# removing all punctuations    \n",
    "\n",
    "# stopwords list, lemmatizer and stemmer\n",
    "stopwords_list=stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "#splitting sntences into words and storing them in place of sentences as list of words\n",
    "for i in range(0,len(change_sentence_list)):\n",
    "    change_sentence_list[i] = change_sentence_list[i].split()\n",
    "print(change_sentence_list)\n",
    "# a list to store unique important words\n",
    "unique_words=[]\n",
    "\n",
    "# lemmitizing , stemming and removing stopwords and digits from words lists\n",
    "for i in range(0,len(change_sentence_list)):\n",
    "    b=[]\n",
    "    for j in range(0,len(change_sentence_list[i])):\n",
    "        change_sentence_list[i][j]=change_sentence_list[i][j].lower()\n",
    "        #change_sentence_list[i][j]=stemmer.stem(change_sentence_list[i][j])\n",
    "        change_sentence_list[i][j]=lemmatizer.lemmatize(change_sentence_list[i][j]) \n",
    "        if change_sentence_list[i][j] not in stopwords_list and change_sentence_list[i][j].isnumeric()!=True:\n",
    "            b.append(change_sentence_list[i][j])\n",
    "            unique_words.append(change_sentence_list[i][j])\n",
    "    change_sentence_list[i]=b   \n",
    "\n",
    "#print(change_sentence_list)\n",
    "\n",
    "\n",
    "#removing the words that are repeated by changin it into set   \n",
    "unique_words = list(set(unique_words))\n",
    "unique_words.sort()\n",
    "#print(unique_words)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>abstractionthe</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abstractive</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>academic</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actually</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>whether</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>without</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>work</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>write</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>158 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  \\\n",
       "abstractionthe   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   \n",
       "abstractive      0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
       "academic         0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
       "actual           0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
       "actually         0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
       "...             ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..   \n",
       "whether          0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   \n",
       "without          0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   \n",
       "word             0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
       "work             0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
       "write            0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   \n",
       "\n",
       "                15  16  17  18  \n",
       "abstractionthe   0   0   0   0  \n",
       "abstractive      1   0   0   1  \n",
       "academic         0   0   0   0  \n",
       "actual           0   1   0   0  \n",
       "actually         1   1   0   0  \n",
       "...             ..  ..  ..  ..  \n",
       "whether          0   0   0   0  \n",
       "without          0   0   0   0  \n",
       "word             1   0   0   0  \n",
       "work             0   0   0   1  \n",
       "write            0   0   0   0  \n",
       "\n",
       "[158 rows x 19 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BAG OF WORDS\n",
    "bag_of_words={}\n",
    "for j in range(0,len(change_sentence_list)):\n",
    "    b=[]\n",
    "    for i in unique_words:\n",
    "        if i in change_sentence_list[j]:\n",
    "            b.append(change_sentence_list[j].count(i))\n",
    "        else:\n",
    "            b.append(0)\n",
    "    bag_of_words[j] = b\n",
    "#print(bag_of_words)\n",
    "    \n",
    "bow_df=pd.DataFrame(bag_of_words,index=unique_words)\n",
    "bow_df\n",
    "\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>abstractionthe</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.944439</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abstractive</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.251292</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.251292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>academic</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.944439</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.944439</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actually</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.251292</td>\n",
       "      <td>3.251292</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>whether</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.944439</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>without</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.944439</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.944439</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>work</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.944439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>write</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.944439</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>158 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0         1    2    3    4    5    6         7         8   \\\n",
       "abstractionthe  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.000000  0.000000   \n",
       "abstractive     0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.000000  0.000000   \n",
       "academic        0.0  3.944439  0.0  0.0  0.0  0.0  0.0  0.000000  0.000000   \n",
       "actual          0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.000000  0.000000   \n",
       "actually        0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.000000  0.000000   \n",
       "...             ...       ...  ...  ...  ...  ...  ...       ...       ...   \n",
       "whether         0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.000000  0.000000   \n",
       "without         0.0  0.000000  0.0  0.0  0.0  0.0  0.0  3.944439  0.000000   \n",
       "word            0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.000000  0.000000   \n",
       "work            0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.000000  0.000000   \n",
       "write           0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.000000  3.944439   \n",
       "\n",
       "                 9    10        11   12        13   14        15        16  \\\n",
       "abstractionthe  0.0  0.0  0.000000  0.0  3.944439  0.0  0.000000  0.000000   \n",
       "abstractive     0.0  0.0  0.000000  0.0  0.000000  0.0  3.251292  0.000000   \n",
       "academic        0.0  0.0  0.000000  0.0  0.000000  0.0  0.000000  0.000000   \n",
       "actual          0.0  0.0  0.000000  0.0  0.000000  0.0  0.000000  3.944439   \n",
       "actually        0.0  0.0  0.000000  0.0  0.000000  0.0  3.251292  3.251292   \n",
       "...             ...  ...       ...  ...       ...  ...       ...       ...   \n",
       "whether         0.0  0.0  3.944439  0.0  0.000000  0.0  0.000000  0.000000   \n",
       "without         0.0  0.0  0.000000  0.0  0.000000  0.0  0.000000  0.000000   \n",
       "word            0.0  0.0  0.000000  0.0  0.000000  0.0  3.944439  0.000000   \n",
       "work            0.0  0.0  0.000000  0.0  0.000000  0.0  0.000000  0.000000   \n",
       "write           0.0  0.0  0.000000  0.0  0.000000  0.0  0.000000  0.000000   \n",
       "\n",
       "                 17        18  \n",
       "abstractionthe  0.0  0.000000  \n",
       "abstractive     0.0  3.251292  \n",
       "academic        0.0  0.000000  \n",
       "actual          0.0  0.000000  \n",
       "actually        0.0  0.000000  \n",
       "...             ...       ...  \n",
       "whether         0.0  0.000000  \n",
       "without         0.0  0.000000  \n",
       "word            0.0  0.000000  \n",
       "work            0.0  3.944439  \n",
       "write           0.0  0.000000  \n",
       "\n",
       "[158 rows x 19 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tf-IDF\n",
    "document_freq=[]\n",
    "for i in range(0,len(unique_words)):\n",
    "    b=0\n",
    "    for j in change_sentence_list:\n",
    "        if unique_words[i] in j:\n",
    "            b=b+1\n",
    "    document_freq.append(math.log(len(change_sentence_list)/b)+1)   \n",
    "\n",
    "tf_idf={}\n",
    "for i in range(0,len(change_sentence_list)):\n",
    "    tf_idf[i] = bow_df[i]*document_freq\n",
    "\n",
    "tfidf_df = pd.DataFrame(tf_idf)\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.124968</td>\n",
       "      <td>0.152297</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032500</td>\n",
       "      <td>0.036517</td>\n",
       "      <td>0.059288</td>\n",
       "      <td>0.230657</td>\n",
       "      <td>0.082584</td>\n",
       "      <td>0.078362</td>\n",
       "      <td>0.036695</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.124968</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.122013</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015059</td>\n",
       "      <td>0.016921</td>\n",
       "      <td>0.158651</td>\n",
       "      <td>0.097184</td>\n",
       "      <td>0.014010</td>\n",
       "      <td>0.134692</td>\n",
       "      <td>0.068014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016724</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016267</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.152297</td>\n",
       "      <td>0.122013</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.046760</td>\n",
       "      <td>0.048523</td>\n",
       "      <td>0.054522</td>\n",
       "      <td>0.185737</td>\n",
       "      <td>0.184849</td>\n",
       "      <td>0.045142</td>\n",
       "      <td>0.132909</td>\n",
       "      <td>0.119843</td>\n",
       "      <td>0.057665</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095708</td>\n",
       "      <td>0.076187</td>\n",
       "      <td>0.052414</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046760</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036284</td>\n",
       "      <td>0.135171</td>\n",
       "      <td>0.025034</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027705</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030429</td>\n",
       "      <td>0.047778</td>\n",
       "      <td>0.055109</td>\n",
       "      <td>0.032330</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.032500</td>\n",
       "      <td>0.015059</td>\n",
       "      <td>0.048523</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.094639</td>\n",
       "      <td>0.080124</td>\n",
       "      <td>0.039446</td>\n",
       "      <td>0.017558</td>\n",
       "      <td>0.019542</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.036517</td>\n",
       "      <td>0.016921</td>\n",
       "      <td>0.054522</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.094639</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.024646</td>\n",
       "      <td>0.044322</td>\n",
       "      <td>0.019729</td>\n",
       "      <td>0.097991</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061648</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.059288</td>\n",
       "      <td>0.158651</td>\n",
       "      <td>0.185737</td>\n",
       "      <td>0.036284</td>\n",
       "      <td>0.080124</td>\n",
       "      <td>0.024646</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.155892</td>\n",
       "      <td>0.020406</td>\n",
       "      <td>0.097273</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.102639</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055259</td>\n",
       "      <td>0.034479</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.230657</td>\n",
       "      <td>0.097184</td>\n",
       "      <td>0.184849</td>\n",
       "      <td>0.135171</td>\n",
       "      <td>0.039446</td>\n",
       "      <td>0.044322</td>\n",
       "      <td>0.155892</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.109383</td>\n",
       "      <td>0.193066</td>\n",
       "      <td>0.044538</td>\n",
       "      <td>0.026569</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.092292</td>\n",
       "      <td>0.045817</td>\n",
       "      <td>0.138901</td>\n",
       "      <td>0.031003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.082584</td>\n",
       "      <td>0.014010</td>\n",
       "      <td>0.045142</td>\n",
       "      <td>0.025034</td>\n",
       "      <td>0.017558</td>\n",
       "      <td>0.019729</td>\n",
       "      <td>0.020406</td>\n",
       "      <td>0.109383</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.066411</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023653</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040788</td>\n",
       "      <td>0.021779</td>\n",
       "      <td>0.125920</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.078362</td>\n",
       "      <td>0.134692</td>\n",
       "      <td>0.132909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019542</td>\n",
       "      <td>0.097991</td>\n",
       "      <td>0.097273</td>\n",
       "      <td>0.193066</td>\n",
       "      <td>0.066411</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.044129</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.036695</td>\n",
       "      <td>0.068014</td>\n",
       "      <td>0.119843</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044538</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044129</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.076689</td>\n",
       "      <td>0.125063</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.162785</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.057665</td>\n",
       "      <td>0.027705</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026569</td>\n",
       "      <td>0.023653</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076689</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.072415</td>\n",
       "      <td>0.067652</td>\n",
       "      <td>0.045141</td>\n",
       "      <td>0.057006</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066042</td>\n",
       "      <td>0.019101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061648</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125063</td>\n",
       "      <td>0.072415</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.055164</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053850</td>\n",
       "      <td>0.041221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.018046</td>\n",
       "      <td>0.016724</td>\n",
       "      <td>0.095708</td>\n",
       "      <td>0.030429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.102639</td>\n",
       "      <td>0.092292</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.067652</td>\n",
       "      <td>0.055164</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.173295</td>\n",
       "      <td>0.215292</td>\n",
       "      <td>0.160838</td>\n",
       "      <td>0.217902</td>\n",
       "      <td>0.073329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076187</td>\n",
       "      <td>0.047778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045817</td>\n",
       "      <td>0.040788</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045141</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.173295</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.125847</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.017553</td>\n",
       "      <td>0.016267</td>\n",
       "      <td>0.052414</td>\n",
       "      <td>0.055109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055259</td>\n",
       "      <td>0.138901</td>\n",
       "      <td>0.021779</td>\n",
       "      <td>0.021109</td>\n",
       "      <td>0.162785</td>\n",
       "      <td>0.057006</td>\n",
       "      <td>0.066680</td>\n",
       "      <td>0.215292</td>\n",
       "      <td>0.125847</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.092286</td>\n",
       "      <td>0.060811</td>\n",
       "      <td>0.064137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032330</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034479</td>\n",
       "      <td>0.031003</td>\n",
       "      <td>0.125920</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160838</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.092286</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066042</td>\n",
       "      <td>0.053850</td>\n",
       "      <td>0.217902</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060811</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.143166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032238</td>\n",
       "      <td>0.061209</td>\n",
       "      <td>0.037617</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043718</td>\n",
       "      <td>0.019387</td>\n",
       "      <td>0.017259</td>\n",
       "      <td>0.050839</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019101</td>\n",
       "      <td>0.041221</td>\n",
       "      <td>0.073329</td>\n",
       "      <td>0.032939</td>\n",
       "      <td>0.064137</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.143166</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "0   1.000000  0.124968  0.152297  0.000000  0.032500  0.036517  0.059288   \n",
       "1   0.124968  1.000000  0.122013  0.000000  0.015059  0.016921  0.158651   \n",
       "2   0.152297  0.122013  1.000000  0.046760  0.048523  0.054522  0.185737   \n",
       "3   0.000000  0.000000  0.046760  1.000000  0.000000  0.000000  0.036284   \n",
       "4   0.032500  0.015059  0.048523  0.000000  1.000000  0.094639  0.080124   \n",
       "5   0.036517  0.016921  0.054522  0.000000  0.094639  1.000000  0.024646   \n",
       "6   0.059288  0.158651  0.185737  0.036284  0.080124  0.024646  1.000000   \n",
       "7   0.230657  0.097184  0.184849  0.135171  0.039446  0.044322  0.155892   \n",
       "8   0.082584  0.014010  0.045142  0.025034  0.017558  0.019729  0.020406   \n",
       "9   0.078362  0.134692  0.132909  0.000000  0.019542  0.097991  0.097273   \n",
       "10  0.036695  0.068014  0.119843  0.000000  0.000000  0.000000  0.000000   \n",
       "11  0.000000  0.000000  0.057665  0.027705  0.000000  0.000000  0.000000   \n",
       "12  0.000000  0.000000  0.000000  0.000000  0.000000  0.061648  0.000000   \n",
       "13  0.018046  0.016724  0.095708  0.030429  0.000000  0.000000  0.102639   \n",
       "14  0.000000  0.000000  0.076187  0.047778  0.000000  0.000000  0.000000   \n",
       "15  0.017553  0.016267  0.052414  0.055109  0.000000  0.000000  0.055259   \n",
       "16  0.000000  0.000000  0.000000  0.032330  0.000000  0.000000  0.034479   \n",
       "17  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "18  0.000000  0.000000  0.032238  0.061209  0.037617  0.000000  0.043718   \n",
       "\n",
       "          7         8         9         10        11        12        13  \\\n",
       "0   0.230657  0.082584  0.078362  0.036695  0.000000  0.000000  0.018046   \n",
       "1   0.097184  0.014010  0.134692  0.068014  0.000000  0.000000  0.016724   \n",
       "2   0.184849  0.045142  0.132909  0.119843  0.057665  0.000000  0.095708   \n",
       "3   0.135171  0.025034  0.000000  0.000000  0.027705  0.000000  0.030429   \n",
       "4   0.039446  0.017558  0.019542  0.000000  0.000000  0.000000  0.000000   \n",
       "5   0.044322  0.019729  0.097991  0.000000  0.000000  0.061648  0.000000   \n",
       "6   0.155892  0.020406  0.097273  0.000000  0.000000  0.000000  0.102639   \n",
       "7   1.000000  0.109383  0.193066  0.044538  0.026569  0.000000  0.092292   \n",
       "8   0.109383  1.000000  0.066411  0.000000  0.023653  0.000000  0.000000   \n",
       "9   0.193066  0.066411  1.000000  0.044129  0.000000  0.000000  0.062531   \n",
       "10  0.044538  0.000000  0.044129  1.000000  0.076689  0.125063  0.000000   \n",
       "11  0.026569  0.023653  0.000000  0.076689  1.000000  0.072415  0.067652   \n",
       "12  0.000000  0.000000  0.000000  0.125063  0.072415  1.000000  0.055164   \n",
       "13  0.092292  0.000000  0.062531  0.000000  0.067652  0.055164  1.000000   \n",
       "14  0.045817  0.040788  0.000000  0.000000  0.045141  0.000000  0.173295   \n",
       "15  0.138901  0.021779  0.021109  0.162785  0.057006  0.066680  0.215292   \n",
       "16  0.031003  0.125920  0.000000  0.000000  0.000000  0.000000  0.160838   \n",
       "17  0.000000  0.000000  0.000000  0.000000  0.066042  0.053850  0.217902   \n",
       "18  0.019387  0.017259  0.050839  0.000000  0.019101  0.041221  0.073329   \n",
       "\n",
       "          14        15        16        17        18  \n",
       "0   0.000000  0.017553  0.000000  0.000000  0.000000  \n",
       "1   0.000000  0.016267  0.000000  0.000000  0.000000  \n",
       "2   0.076187  0.052414  0.000000  0.000000  0.032238  \n",
       "3   0.047778  0.055109  0.032330  0.000000  0.061209  \n",
       "4   0.000000  0.000000  0.000000  0.000000  0.037617  \n",
       "5   0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "6   0.000000  0.055259  0.034479  0.000000  0.043718  \n",
       "7   0.045817  0.138901  0.031003  0.000000  0.019387  \n",
       "8   0.040788  0.021779  0.125920  0.000000  0.017259  \n",
       "9   0.000000  0.021109  0.000000  0.000000  0.050839  \n",
       "10  0.000000  0.162785  0.000000  0.000000  0.000000  \n",
       "11  0.045141  0.057006  0.000000  0.066042  0.019101  \n",
       "12  0.000000  0.066680  0.000000  0.053850  0.041221  \n",
       "13  0.173295  0.215292  0.160838  0.217902  0.073329  \n",
       "14  1.000000  0.125847  0.000000  0.000000  0.032939  \n",
       "15  0.125847  1.000000  0.092286  0.060811  0.064137  \n",
       "16  0.000000  0.092286  1.000000  0.000000  0.000000  \n",
       "17  0.000000  0.060811  0.000000  1.000000  0.143166  \n",
       "18  0.032939  0.064137  0.000000  0.143166  1.000000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# COSINE SIMILARITY MATRIX\n",
    "\n",
    "sim_matrix={}\n",
    "for i in range(0,len(change_sentence_list)):\n",
    "    b=[]\n",
    "    for j in range(0,len(change_sentence_list)):\n",
    "        a = np.dot(tfidf_df[i],tfidf_df[j])\n",
    "        a = a/(np.linalg.norm(tfidf_df[i])*np.linalg.norm(tfidf_df[j]))\n",
    "        b.append(a)\n",
    "    sim_matrix[i]=b\n",
    "sim_matrix=pd.DataFrame(sim_matrix)\n",
    "sim_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 18, 1: 20, 2: 19, 3: 11, 4: 10, 5: 10, 6: 12, 7: 16, 8: 15, 9: 14, 10: 7, 11: 10, 12: 15, 13: 16, 14: 5, 15: 17, 16: 12, 17: 4, 18: 19}\n"
     ]
    }
   ],
   "source": [
    "sent_ranks={}\n",
    "for i in range(0,len(change_sentence_list)):\n",
    "    a=0\n",
    "    for j in range(0,len(unique_words)):\n",
    "        a=a+bow_df[i][j]\n",
    "        \n",
    "    sent_ranks[i] = a\n",
    "print(sent_ranks)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 20), (2, 19), (18, 19), (0, 18), (15, 17), (7, 16), (13, 16), (8, 15), (12, 15), (9, 14), (6, 12), (16, 12), (3, 11), (4, 10), (5, 10), (11, 10), (10, 7), (14, 5), (17, 4)]\n",
      "\n",
      " Summarization is the task of condensing a piece of text to a shorter version, reducing the size of the initial text while at the same time preserving key informational elements and the meaning of content.\n",
      "\n",
      " Since manual text summarization is a time expensive and generally laborious task, the automatization of the task is gaining increasing popularity and therefore constitutes a strong motivation for academic research.\n",
      "\n",
      " There are important applications for text summarization in various NLP related tasks such as text classification, question answering, legal texts summarization, news summarization, and headline generation.\n",
      "\n",
      " So, in this case, every line and word of the summary actually belongs to the original document which is summarized.The abstractive approach involves summarization based on deep learning.\n",
      "\n",
      " It has been observed that extractive summaries sometimes work better than the abstractive ones probably because extractive ones don’t require natural language generations and semantic representations.\n"
     ]
    }
   ],
   "source": [
    "sent_ranks= sorted(sent_ranks.items(), key=lambda x: x[1],reverse=True)    \n",
    "print(sent_ranks)\n",
    "imp_sent=[]\n",
    "for i in range(0,5):\n",
    "    imp_sent.append(sent_ranks[i][0])\n",
    "imp_sent.sort()\n",
    "for i in range(0,5):\n",
    "    print(\"\\n\",sentence_list[imp_sent[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "447fb23058b70573e627dd5d588ab11ba5b58f5c4364b00b8b9a3ef40c40dccf"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
